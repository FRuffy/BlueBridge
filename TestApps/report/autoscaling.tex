%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Auto-Scaling}
\label{sec:autoscaling}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Why Distributed Shared Memory?}

Given the results from the scalability analysis, we aim to build a system that
transparently scales for practitioners that desire performance, yet want minimal
changes to their single-threaded interface.

What options exist for this setting? Implementing a full MPI-style system would
allow for the highest potential gains in performance, but is not generalizable
and would be intrusive to the programmer's interface. One could also opt for a system
such as Naiad ~\cite{Murray:2013:NTD:2517349.2522738} or 
Hadoop ~\cite{Vavilapalli:2013:AHY:2523616.2523633}. While simplier to reason about, 
this also requires significant setup and is still requires a large change in the 
programming interface. 

Instead, we turn to the next-simplest abstraction: distributed shared memory (DSM).
DSM comes with the advantage that its programming model is identical to the
single machine (cite this), and hides the complexities of the network from the
developer. DSM seems like an ideal solution for the problem presented, but has
traditionally been shown to lack the required performance, and gives the developer
little control over locality. 

Thus, we propose BlueBridge, a locality aware DSM system. BlueBridge includes
an API for access and placement of distributed shared memory, which aims to
alleviate the concerns previously posed by DSM. With current trends in rack
scale datacenters, disaggregated datacenters are evolving into dense memory
systems with terabit-level internal bandwidth. These environments support the 
conceptual idea of DSM, in which a rack scale system poses as a single machine. 

\subsection{BlueBridge Design}

BlueBridge is designed for a simple interface, that hides the complexity of
shared memory from the developer. The intended users of BlueBridge are those who
have a need to process large datasets, but do not want to reason about the
complexities of distributed shared memory. We choose to provide an interface
to BlueBridge through Python, 

We simulate a DSM system by using Python 2.7, and its built-in \textit{Manager}
library. The \textit{Manager} library allows for simple management of shared
state between processes by providing interfaces to shared arrays, dictionaries
and easy locking semantics. 

Continuing the theme from the COST investigation, we choose to target graph
processing as an example use case for BlueBridge. We also use iGraph 0.6, which
includes a flexible interface for graph processing in Python. iGraph performs
its computation in C and is interfaced with Python, so it came to be a natural 
choice given the design constraints.

\subsection{Experimental Setup}

In order to evaluate the parallel graph processing of BlueBridge, we computed 
PageRank on three graphs, collected from the Stanford Large Network Dataset 
Collection. ~\cite{snapnets} These graphs will be referred to as:

\begin{itemize}
	\item wikiVote: A Wikipedia voter network, with 7115 vertices and 103689 edges
	\item GrQc: A citation network of General Relativity/Quantum Cosmology papers, with 5242 vertices and 124496 edges
	\item condMat: A citation network of Condensed Matter papers, with 23133 nodes and 93497 edges
\end{itemize}

What is most important of these three graphs is that they have varying degrees of
connectivity. GrQc can be split into 2,4 or 8 partitions with a relatively low degree
of cutting, while wikiVote is much more difficult to cut.

\begin{table}[h]
\begin{tabularx}{\linewidth}{|X|X|X|X|}
\hline
Graph & 2 part. & 4 part. & 8 part. \\ \hline \hline
GrQc & 3.38\% & 4.93\% & 7.20\% \\ \hline
condMat & 6.56\% & 13.38\% & 18.69\% \\ \hline
wikiVote & 25.45\% & 50.05\% & 64.79\% \\ \hline
\end{tabularx}
\caption{Percentage of edges cut when splitting into 2,4 or 8 partitions.}
\end{table}

\subsection{Partitioning}

Graph partitioning occurs as a pre-processing step when performing graph processing
on DSM. In order to respect locality within DSM, our partitioning algorithm aims to 
provide an optimal environment for the parallel processing of graphs. We define an 
optimal partitioning scheme as one in which the number of vertices are roughly 
equal for each partition, while the number of edges that cross partitions is 
minimized. We leverage the METIS ~\cite{Lasalle:2013:metis} library, an MPI 
implementation for parallel partitioning of large graphs, to determine an optimal 
partitioning for the system. METIS simply accepts an adjacency list, which can be 
built using iGraph library funcitons, and outputs the the graph partitioning
scheme as a membership vector. METIS is built in and runs in C, and the corresponding
Python binding, PyMetis ~\cite{pymetis} was used to determine the optimal graph 
partitioning once it is read into memory. The time to partition the graph is
several orders of magnitude less than the total processing time.

\begin{table}[h]
\begin{tabularx}{\linewidth}{|X|X|}
\hline
Graph & Time To Partition (avg.) \\ \hline \hline
GrQc & 0.027s \\ \hline
condMat & 0.151s \\ \hline
wikiVote & 0.050s \\ \hline
\end{tabularx}
\caption{Time taken to partition graph with PyMetis}
\end{table}

Naturally, partitioning a graph will cause edges in the graph to cross 
partitions. Applying techniques seen in ~\cite{Tian:2013:thinklikeagraph} and 
~\cite{Chen:2015:powerlyra}, we use a "mirroring" technique for storing edges
between partitions. For each edge that crosses partitions, only vertices on 
two separate partitions will be affected. The partition which contains the 
destination vertex will be considered the master of the given edge. All edges 
which connect to the master vertex from a different partition will create a
"mirror" vertex. These edges are stored in shared memory, such that they can
be written to and read from different machines. Master vertices maintain 
references to the shared edges on which they depend.

\begin{figure}[h]
\includegraphics[width=\linewidth]{"fig/partition_example"}
\caption{An example of the partitioning procedure. Shared edges are loaded into shared memory.}
\end{figure}

\subsection{Loose Synchronization}



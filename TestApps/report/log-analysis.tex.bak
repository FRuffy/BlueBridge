%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Log analysis}

\label{sec:log-analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section overviews the process by which the node logs are merged
together. How the states of independent nodes are combined into
distributed program points and how Dinv infers distributed data
invariants from these combined states.

Inferring valid distributed data invariants is a multi-faceted
problem. The majority of the challenges arise from concurrency. Nodes
which undergo concurrent state changes may simultaneously reach a
state which falsifies an invariant property, only to return to a state
in which the invariant is satisfied upon communicating again. In such
cases the invariant is falsified, but without a mechanism for
reasoning about partial ordering the property cannot be checked. The
following is a list of challenges inherent to checking distributed
invariants which our techniques address.


\begin{enumerate}
%
    \item The number of partially ordered \emph{events} are
        exponential in the number of nodes. In order to reason about
        the full set of partial orderings a complete model of this
        space must be constructed.
%
    \item Distributed state is not always resident on the nodes of the
        system. Analysis on distributed state is incomplete if any
        state is in transit.
%
    \item Distributed systems are highly variable in their both their
        communication patters, topologies, and node hierarchies.
        Determining the granularity at which to test for distributed
        invariants is therefore system, and property dependent.
%
\end{enumerate}
%
    Without using any mechanism for reasoning about the order of
    \emph{event}s in a distributed system, the number of possible
    \emph{event} orderings is exponential. Testing invariant
    properties on every interleaving combination of events would not
    only be extremely costly, but also produce invariants inconsistent
    with the systems execution. Vector clocks establish a partial
    ordering on events between communicating nodes. However, the
    growth of partial orderings is still exponential on events which
    occur between communication. Figure~\ref{fig:time-lattice}A shows
    the execution between two Serf nodes $N_0$ and $N_1$ from
    Figure~\ref{fig:gossip-execution}. $N_0$ sends a \emph{ping},
    executes two local events, then receives an \emph{ack} message.
    Figure~\ref{fig:time-lattice}B shows a lattice structure of all
    possible partial orderings of events which respect the
    happens-before relationship established by vector clocks. We make
    use of the lattice of partial orderings as a model for accurately
    describing a distributed execution.  Algorithm~\ref{alg:lattice}
    is a high level overview of our lattice construction algorithm.

    Using a lattice to model distributed execution provides a complete
    view of the execution. However, not every point on the lattice is
    appropriate for testing invariants. Distributed state has two
    possible locations, either resident on a node, or in transit on a
    wire. In the latter case the state of the system is unobservable.
    Lattice points at which state is in transmission provide only a
    partial view of distributed state. Testing invariants at such
    points would be incorrect as state which could falsify a property
    may not be present. \emph{Consistent cuts} of a network are points
    in relative time at which no messages are in transit. Within the
    lattice \emph{consistent cuts} can be computed by enumerating sent
    and received messages. We use the set of consistent cuts present
    in the lattice as a global granularity for testing invariants.
    Merging the states of interacting nodes at the granularity of
    consistent cuts guarantees that the sum of node states is
    representative of the state of the system.
    Figure~\ref{fig:time-lattice}B shows all lattice points which are
    \emph{Consistent cuts} highlighted in red.
    Algorithm~\ref{alg:mineCuts} shows our process for extracting
    \emph{Consistent cuts} from a lattice.

    Distributed systems are designed for a variety of purposes.  Their
    behaviour, communication patterns, and desired invariants are also
    highly varied. For example leader election algorithms, and
    peer-to-peer systems typically have multiple nodes running
    identical code, whereas client-server systems have distinct
    functionality.  Invariant properties hold at differing
    granularity's subject to the architecture of the system. No one
    strategy for testing invariants would detect every desired
    property in arbitrary systems.  Our approach to this problem makes
    use of 3 heuristics for testing invariants at differing
    granularity's of the systems execution.

    
    In this section we describe the structures and processes we use to
    collect the set of all consistent cuts from an execution log,
    merge the loged states of nodes into distributed program points,
    and test merged states for invariants. The following is a high
    level overview of our process.


\begin{enumerate}

\item Construct a lattice structure which models all cuts of the log
  such that the happened-before relation is respected in each cut.

\item Enumerate sent and received messages for each node in the log,
  and reduce the lattice to cuts with no outstanding messages, the
  reduced lattice is the set of all consistent cuts.

  \item Apply a merging strategy to a consistent cut to produce sets
      of distributed program points. Finally distributed program
      points are bucketed together based on nodes they are composed
      of, and the variables they contain.

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]
    \KwData{A system log $L$}
    \KwResult{A set of consistent node states $S$ }
        $clocks$ := vectorClockArraysFromLogs($L$)\\
        $lattice$ := buildLattice($clocks$)\\
        $deltaComm$ := enumerateCommunication($clocks$)\\
        $cuts$ := mineConsistentCuts($lattice$, $clocks$, $deltaComm$)\\
        $S$ := statesFromCuts($cuts$, $clocks$, $logs$)\\
    \Return $S$
    \label{alg:mineStates}
    \vspace{2mm}
    \caption{High level log merging overview}

\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Lattice}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


A lattice contains all event orderings consistent with the
happens-before relation~\cite{Cooper:1991:CDG:122759.122774}. This
property constrains the exponential set of potential event orderings,
and supplies a complete view of a systems execution.

A variety of different algorithms for constructing a lattice have been
proposed~\cite{Garg}. Algorithm~\ref{alg:lattice} presents the lattice
construction algorithm in its most general form. Each level of the
lattice is constructed sequentially.  All clocks from the previous
level have their values incremented by 1 for each node. If the
incremented clock value is valid under the happens-before relation, it
is appended to the next level of the lattice.
Section~\ref{sec:lattice-appendix} contains an example lattice.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]

    \KwData{Set of Host traces $T$}
    \KwResult{Lattice representation of a System Trace $S$}
    i = 0 \;
    Level[i].append(ZeroClock())\;
    \While{!Level[i].empty()} {
        i++\;
        \For{Clock $\in$ Level[i-1]} {
            \For{Host $\in$ T}{
                Clock.increment(Host)\;
                \If {ValidLatticePoint(T,Clock)}{
                    Level[i].append(Clock)
                }
            }
        }
        $S$.append(Level[i])
    }
    \vspace{2mm}
    \caption{Lattice Construction Algorithm}
    \label{alg:lattice}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Calculating consistent cuts}

A variety of algorithms exist for computing consistent cuts from the
execution of a distributed system~\cite{MATTERN1993423}. A high level
discription of our algorithm is given here, a full overview is given
in Section~\ref{sec:consistent-cuts-appendix}. We process the log by
enumerating sent and received messages and maintianing a delta for
each node. Points in the lattice on which the sum of each nodes delta
equals zero are to consitent cuts. This calculation is run on every
point in the lattice, the result is a complete set of consistent cuts.
Finally the states of nodes at the logical time corresponding to a
consistent cut are collected together. The output of this process is
the complete set of node states at every point during execution when
all state was resident in memory. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Host State Merging Strategies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    To check state invariants on multiple nodes, their states are
    merged together and their variables are tested for invariants. All
    logged states are related to the program points the were logged
    on. Therefore we define a merged set of states to be a
    \emph{distributed program point}. Determining which states to
    merge when checking for particular invariants is difficult because
    distributed systems come in a wide variety of structures. Leader
    election algorithms for example have distinct node states which
    specify their behaviour. Distributed hash tables typically consist
    of identical peers whose behaviour is dictated by a unique
    identifier. Client server architectures are centralized and
    typically execute very distinct code. In each case the systems
    invariant properties hold at different levels, and require
    alternative views of the execution to test them. Checking that all
    nodes in an election agree on a result, or that all hash peers
    agree on where to route for a hash require a system wide view.
    Testing that a server responds idempotently to client requests
    only requires a view between client and server. Checking that
    state updates are propagated through a gossip protocol requires a
    view specific to the dataflow the the system. No one strategy for
    testing distributed invariants will check the desired property in
    an arbitrary system. Our solution is 3 heuristics for merging node
    state in order to test invariants. Firstly we propose merging the
    states of all nodes in a consistent cut, secondly merging only the
    states of senders and receivers, and lastly merging the states of
    nodes which participated in totally ordered communication.
    

    

\begin{figure}[h]

    \includegraphics[width=0.5\textwidth]{fig/gossip-execution}

    \caption{Sample Serf execution with 4 nodes running code
        from Figure~\ref{fig:data-flow}. This diagram is a subset of a larger
        execution. Points $L1$ and $L2$ on each nodes timeline
        represent the execution of two separate logging functions.
    Dashed lines $C1$, $C2$, $C3$ are consistent cuts of the execution
including only 4 nodes.} 

    \label{fig:gossip-execution}

\end{figure}

\begin{figure}[h]
    \includegraphics[width=0.5\textwidth]{fig/cuts-to-dpp}

    \caption{Diagram of the various merging strategies converting cuts
        into sets of distributed program points. On the left, are cuts
        $C1$, $C2$, $C3$ from Figure~\ref{fig:gossip-execution}
        depicted as sets of logged state. On the right depicts how the
    different merging strategies \textbf{WCM}: whole cut merge,
\textbf{SR}: send receive merge, and \textbf{TO}: total order merge,
produce different distributed program points. Highlighted are two
matching distributed programs points.}

\label{fig:cuts-to-dpp}

\end{figure}

\textbf{Whole Cut Merge}  This merge is performed by collecting the
states of all hosts in a consistent cut and testing them for
invariants together.  Invariants which are true for all nodes on all
consistent cuts are checked at this granularity. For example in a
distributed hash table where resources are not duplicated across
multiple nodes an invariant such as $\forall i,j \in nodes$ $i-keys !=
j-keys$ would be detected.  In other cases whole cut merge will not
detect invariant properties.  Figure~\ref{fig:cuts-to-dpp} shows the
whole cut merged points from a Serf execution. Checking Serfs eventual
consistency invariant with whole cut merge would not work. In the
example if the whole cut merge distributed program point from $C1$ and
$C3$ were checked for invariants \dinv would detect that $N_0-Events
!= N_3-Events$ because the gossip message from $N_3$ had yet to
propagate to $N_0$. Whole cut merge falsifies Serfs consistency
invariant because at the granularity of a consistent cut the property
does not hold.

\textbf{Send-Receive Merge} In some cases it is nessisary to analyze
the states of pairs of interacting nodes. In such cases merging the
states of senders and receivers in a consistent cut can be used to
verify invariant properties of their isolated interaction.
Figure~\ref{fig:cuts-to-dpp} details how send receive merge constructs
distributed program point. The state of a sender immediately before a
send is merged with the state of the receiver immediately after
receiving. \textit{Cut 3} details how states are merged in cuts where
nodes both sent and received messages, as well as nodes which did not
communicate. In the case of no communication the node state is left
isolated. Whole cut merge fails to detect eventual consistency, but
the invariant can be verified with send-receive merge. In the case of
Figure~\ref{fig:cuts-to-dpp} the testing the send receive distributed
program points would result in the invariants $N_3-Events ==
N_2-Events$, $N_2-Events == N_1-Events$, and $N_1-Events ==
N_0-Events$ because at every point at which nodes communicate the
invariant holds. Send-Receive merge is useful for detecting fine grain
invariants, but because the state of all nodes is not analyzed
together the properties lack generality, requiring users to confirm
the invariant between every pair of nodes.

\textbf{Total Ordering Merge} It is often useful to delineate between differing
messaging protocols between nodes. In such cases merging node states which
participate in a totally ordered interaction is useful. Vector clock values
within a consistent cut form a directed acyclic graph (DAG). A totally ordered
set of clock values can be extracted by starting at the most recent clock in
the cut, and backtracking along the first incident edges. When backtracking
ends at a terminal node of the DAG, the path from the most recent clock to the
terminal node, is a total ordering. The total ordering can be removed from the
cut, and what remains is a sub cut of vector clocks, which is also a directed
acyclic graph. This process can be performed iteratively to extract all total
orderings within the cut, Algorithm~\ref{alg:dpp} lists of this procedure.

 The set of all distributed program points is computed by running this
 algorithm on the set of all consistent consistent cuts.
 Figure~\ref{fig:cuts-to-dpp} details how this merging strategy
 differs from send receive merge.  Specifically in \textit{Cut 3} the
 states of the three totally ordered nodes are merged together. Total
 ordering merge has the same ability as send receive merge to detect
 eventual consistency in Serf, but it detects it in a broader scope.
 In the case of \textit{Cut 3} the invariant $N_3-Events == N_2-Events
 == N_1-Events$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}
 \KwData{$State$}
 \KwResult{Set of distributed program points within a state $DPP's$}
 
 $clocks$ = $s$.getClocks()\\
 $dag$ = DagFromClocks($clocks$)\\
 \While {$dag$.Root != nil} {
     $path$ = $dag$.BacktrackFromRoot()\\
     $point$ = getHostStatesFromClocks($State$,$path$)\\
     $dag$.extract($path$)\\
     $DPPs$.append($point$)\\
 }
 \Return{$DPPs$} \\
 \vspace{2mm}
 \caption{Extract the set of distributed program points from a consistent distributed state}

 \label{alg:dpp}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \textbf{Bucketing}:   To derive arbitrary invariants multiple
 instances of variables and their values are required. Properties
 which hold on all instances are invariant. To test invariants on the
 state of two or more nodes the combined set of variables must be
 identical in each instance that an invariant is tested. Testing
 invariants on instances with non-identical sets of variables would
 identify correct invariants on the intersection of the sets, and
 potentially incorrect invariants on the sets symmetric difference.
 Because of this we only test invariants on identical sets of states.

 The final process to prepare the logs for Daikon is to bucket
 distributed program points together.  Distributed program points are
 identifiable by the triple \emph{(nodeid, source, loc)} of the
 logging statements from which generated them, and the set of nodes
 from which are merged.  Figure~\ref{cuts-to-dpp} highlights in orange
 two distributed program points generated by whole cut merge from
 different points in Serfs execution which would be bucketed together.
 The Green highlighted points generated by send receive merge
 correspond to the same distributed program point merged from separate
 cuts, these points are also bucketed together.  We consider all dump
 statements to be unique as they do not necessarily contain the same
 set of variables. Therefore, regardless of merging strategy the
 number of different buckets generated can be exponential in the
 number of \textit{Dump} statements. \textit{Dump} statements provide
 a precise view of particular functionality, but also cause an output
 explosion.  \textit{Track} consolidates each statement into one, the
 result is a collapse of exponential output at the cost of precision.
 For example using whole cut merge and 2 \textit{Dump} statements on a
 log of 4 nodes results in a total of 16 distinct distributed program
 points, whereas \textit{Track} in conjunction with single cut merge
 results to 1 distributed program point. This exponential reduction is
 true for all merging strategies. 

The bucketing algorithm is as follows: distributed program points are
placed into buckets using their id's as identifiers.  Finally each
bucket is written to a separate file for analysis by Daikon.

%\todo{add an example of bucketing}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Daikon}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Daikon is a tool which dynamically detects likely data
invariants~\cite{Ernst07}.  Daikon infers likely data invariants based
on data traces, referred to as \emph{dtraces}.  The result of running
DInv on a log, is a set of \emph{dtraces} for each unique distributed
program point present during the systems execution.  DInv output a
dtrace file for each set of matching distributed program points.
Daikon is run on each \emph{dtrace} to detect invariants at the
corresponding point, Daikon's output is a set of invariants with
variables prefixed by the nodes id, source file, and line of code on
which they were present to distinguish them from one another.
Section~\ref{dikon-appendix} details Daikon's approach to invariant
detection, and our methodology for translating Daikon invariants into
first order logic.


Thank you for submitting your paper to The 14th USENIX Symposium on
Networked Systems Design and Implementation (NSDI '17).

       Title: Checking distributed systems using likely state invariants
     Authors: Stewart Grant (University of British Columbia)
              Hendrik Cech (University of Bamberg)
              Ivan Beschastnikh (University of British Columbia)
  Paper site: https://nsdi17.usenix.hotcrp.com/paper/323?cap=0323ahMn_NV2L8vo

Reviews and comments on your paper are appended to this email. The
submission URL above also has the paper's reviews and comments.

-Aditya and Jon
nsdi17chairs@usenix.org

===========================================================================
                           NSDI '17 Review #323A
---------------------------------------------------------------------------
  Paper #323: Checking distributed systems using likely state invariants
---------------------------------------------------------------------------

                      Overall merit: 2. Weak Reject (This paper doesn't
                                        belong in the conference, but it
                                        won't upset me.)
                 Reviewer expertise: 2. Some familiarity

                         ===== Paper summary =====

This paper presents Dinv, an approach to help developers uncover the runtime distributed state properties of distributed systems.  Dinv uses both static and dynamic analysis to infer relationships between variables at different nodes and thus construct likely state invariants.  Dinv works on systems implemented in Go and has been applied to etc Raft, the SWIM protocol in Serf, Taipei-Torrent, and Groupcache.  They find a number of (likely) invariants in each system.   They show that the initial static analysis time, the logging overhead, and the dynamic analysis of the resulting traces are all reasonable.

                           ===== Strengths =====

+ An interesting problem area
+ Maybe this is the first step towards a more complete solution
+ Experience with 4 distributed systems written in Go

                     ===== Areas for improvement =====

- The authors need to show a case where Dinv uncovers something genuinely useful

- Be able to handle cases containing failures; corner-cases would be exactly the circumstances in which invariants may not hold and so these need to be checked the most rigorously

                      ===== Comments for author =====

I think this is an interesting problem and perhaps a good first step.  However, I have a number of concerns with the current system and paper submission.

I have a hard time seeing where this approach will be useful.  Dinv cannot be used to prove that state invariants hold -- it can only show *likely* invariants that held for the program instances that it observed.   Certainly, if Dinv could "prove" that an invariant is true, then that would be worthy.

Given that it can't, I would like to see case studies where Dinv did give useful information.  For example, in the Introduction, the authors say that it could be used for test-case generation to drive the system towards states that violate an inferred invariant; do you have examples of this?  Can you quantify how much it would simplify test-case generation?  Can you show then actually finding violations?

You also say in the Introduction that it could help with debugging if an inferred invariant actually violated the developer's mental model.  Have you found any cases of this?

To demonstrate the usefulness of Dinv, an interesting case study to explore could be how to use Dinv to better understand when an expected invariant does NOT hold -- since those are the problematic cases.  How will Dinv fail in this case?  What information will the developer see?

Could you construct a case study where you modify the code base to violate a invariant in some cases and then both 1) quantify how often Dinv *incorrectly* thinks the variant still holds and 2) show how the developer could use these results to repair the invariant?

Again, as stated in the author's discussion, I think that the inability of Dinv to handle failure cases is a serious fundamental problem.

The process for applying Dinv to existing systems is not entirely clear to me.  The description in Section 5 makes it sound like the developer needs to be very sophisticated to understand how to refine the analysis -- to determine which variables to log when and where and to know which merge to use (whole cut vs send-receive vs total ordering).

Can you quantify the amount of developer work/time that was required to analyze each of your 4 distributed systems?

Smaller point: did you find circumstances where total ordering merge was needed?

===========================================================================
                           NSDI '17 Review #323B
---------------------------------------------------------------------------
  Paper #323: Checking distributed systems using likely state invariants
---------------------------------------------------------------------------

                      Overall merit: 3. Weak Accept (I can't complain about
                                        this paper being accepted, but I'm
                                        not enthusiastic.)
                 Reviewer expertise: 1. No familiarity

                         ===== Paper summary =====

Dinv is a tool for automatically inferring distributed system invariants and
checking them against execution. The tool collects distributed logs with vector
clocks, extracts consistent cuts, and then mines all the cuts for interesting
invariants. The cuts and the invariant extraction is done via three different
methods, and can incorporate user annotation.

Dinv has been applied on several real-life codebases, e.g., Hashicorp's serf
(a SWIM implementation), Raft (Paxos consensus), Kademlia (DHT), and others. The paper presents
their evaluation. The invariants checked via Dinv are the key
safety invariants of the respective protocols, e.g., leader dominance in Raft,
or eventually consistent membership view in serf. The paper presents performance
evaluation of running Dinv on traces from these programs that represent
reasonably sizable executions.

                           ===== Strengths =====

important area to tackle

convincing evaluation

                     ===== Areas for improvement =====

It is difficult for non-experts to understand the paper.

                      ===== Comments for author =====

It is difficult for non-experts to understand the paper. The main contribution
is the reduction of search space of all cuts checked against invariants, but the
paper tells us very little about this.

Also, I, for one, am stuck already at Figure 3. Something seems wrong here, either the lattice has vector
clocks with node 0 and 1 places swapped, or I completely don't understand it.
For example, [2,1] is not a valid point, since it violates the happens before
relation (node 0 sends a message at step 1, node 1 receives it only at step 2).
In fact, even [1,1] is not valid.

I don't understand the difference between valid lattice points and
strongly consistent cuts. Aren't all cuts that satisfy the happens-before
relation also consisten?

It would help a lot to make the paper self-sufficient by showing a simple
example (like figure 3), explaining cuts, consistent cuts, invariant, and the reduction
techniques on them.

===========================================================================
                           NSDI '17 Review #323C
---------------------------------------------------------------------------
  Paper #323: Checking distributed systems using likely state invariants
---------------------------------------------------------------------------

                      Overall merit: 1. Strong Reject (I'll argue against
                                        this paper.)
                 Reviewer expertise: 3. Knowledgeable

                         ===== Paper summary =====

This paper describes Dinv, a tool for automatically discovering invariants of distributed systems. The tool works like this: (i) it uses program slicing to identify code fragments that depend on network communication; (ii) it instruments that code to produce logs annotated with vector timestamps; (iii) it uses heuristic merge functions to compute distributed snapshots; and (iv) it uses Daikon, an existing sequential invariant detection tool, to produce likely invariants. Dinv has been applied to several programs written in Go, and was able to discover invariants that seem sensible.

                           ===== Strengths =====

* A tool like Dinv could make it much easier to develop distributed systems, and reduce the chance of bugs.

* The overall workflow based on slicing, instrumentation, and mining, seems sensible.

* A prototype has been implemented and successfully applied to real-world applications.

                     ===== Areas for improvement =====

* The paper's main innovation is quite narrow, in the merge functions. It's not clear whether these functions are fundamental or what properties they enjoy. Otherwise, most of the system is constructed from stock components or other tools.

* It's somewhat unusual for the evaluation of a bug finding tool to not find any bugs! Since the tool is fundamentally incomplete, it's unclear if this is a positive or negative.

* The paper classifies a large chunk of the literature on specification mining because it "focuses on events, and not state." But this distinction does not seem fundamental since in a message-passing system, events can be used to observe state. It's not clear to me what the innovation over some of these papers is. For example, the work on mining temporal properties that is cited [2] seems much more likely to capture useful invariants -- if only because distributed systems tend to exhibit interesting temporal properties vs. non-temporal state properties.

                      ===== Comments for author =====

Dinv tackles a difficult and important problem. And while my overall score is low, I am enthusiastic about this work. The paper would be stronger if it did one of two things (i) developed a significant new conceptual idea or (ii) demonstrated that the tool is able to find bugs that would be difficult or impossible for a human to find. Unfortunately in its current form, it doesn't succeed in either of these. The paper's main contribution is a somewhat small set of heuristic merge functions and the evaluation seems inconclusive -- I would have expected to discover some bugs, not simply finding invariants that seem sensible.

Other questions and ideas for strengthening the paper:

* Since you are using a dynamic instrumentation, would it also be interesting to perturb the scheduler? That might make it easier for you to guide the program into states that are likely to break invariants. Likewise, since you control the network would it be interesting to drop messages etc.?

* In the same vein, I recall that Daikon uses unsound techniques to find invariants. Can the same principle be applied to distributed programs in an interesting way?

* Why are state invariants the right formalism for capturing properties of distributed systems? As hinted above, it seems like temporal properties would be much better, and there is already related work on that.

* Can you formalize what properties a "good" merge function has? The ones presented in the paper seem sensible, but I didn't have a sense of their requirements.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}%Do we want this to be at the beginning or end of the paper? Right now this reads more like motivation
\label{sec:related}
% \ugh{ASSIGNED TO: Amanda and Fabian}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We categorize related work to BlueBridge in three categories: distributed shared memory systems, remote paging systems, and using Software Defined Networking for storage or memory.

\textbf{Distributed Shared Memory.} 
% \ac{Should include modern solutions $\rightarrow$ FARM}
% What solutions have been done and proposed in this space? Why is ours supposed to work better than theirs? 
Distributed shared memory has been explored extensively in previous decades~\cite{treadmarks, gms, Protic1996, Nitzberg1991, grappa, farm}. Our solution is similar to traditional distributed shared memory implementations but differs fundamentally in the management strategy. Therefore, we restrict our discussion of related works to the management components of distributed shared memory systems.

Two of the most recent systems are FaRM from Microsoft\cite{farm} and Grappa from University of Washington~\cite{grappa}. Both are similar to the traditional approaches described in~\cite{Protic1996, Nitzberg1991} but differ in the hardware used and design decisions. Both Grappa and FaRM take a more decentralized approach to their directory services. Each node is required to contact the ``owner'' of the memory in order to get routing information or to send requests~\cite{grappa, farm}. This solution tends to avoid the performance overheads from a centralized solution, but can perform poorly when memory migrates. Our solution differs as the memory address is the IPv6 location of the memory. The switch will map that IPv6 address to the correct machine which owns that piece of memory. In the case of memory migration, the controller will update the switch with new rules, thus re-routing the same IPv6 address to the new owner.

% Grappa has a similar approach as FaRM, where their management is decentralized and relies on ownership. All requests must go through the owner of the memory. 

% FARM - RDMA based shared memory space, uses consistent hashing to determine which machine has what data. Needs to query that machine to build the RDMA request. Cluster membership is handled by Zookeeper. We differ because we move the directory service on to the route path. The request must go through the switch anyways, so might as well have the directory look up be at the switch.

% Grappa - decentralized management approach

\textbf{Remote Paging.}
% \ac{Make a point to say most of this work is not \textbf{shared}. Different implementation of directory service (moved fully into the network).}
Remote paging works almost identically to typical operating system paging, except instead of going to disk, it fetches the data from a remote memory source. A lot of work has been done exploring this area, generally, it is found that remote paging tends to be faster than disk accesses when a workload's data cannot fit entirely on its local RAM~\cite{infiniswap}. Two remote paging solutions similar to ours leverage the network~\cite{infiniswap, Liang2005} to improve performance. Infiniswap uses RDMA and Infiniband and achieves better performance on a set of unmodified applications using decentralized placement and eviction. Our solution differs from Infiniswap in two ways: 1) the memory we handle is \textit{shared} as well as remote, and 2) we not only leverage the network, but integrate with it. Infiniswap leverages the network for their implementation, but does not ascribe any intelligence to it, we actually attempt to move intelligence into the network for performance gain and complexity reduction.  

\textbf{Software-Defined Networking for Storage/Memory.}
In the past years, several systems have utilised SDN to shift traditional host-based management into the network. SwitchKV~\cite{switchkv} uses programmability to implement cache look-up directly on the the switch. The controller places table entries containing information whether data is present in the cache node. If not, requests will be routed accordingly and subsequently cached on the node as well as the switch table.
Our use of software-defined networking is similar to SwitchKV as we leverage the global knowledge benefits of the control plane. However, we do not only re-route packets, but incorporate any form of management directly in the networking layer. This may also include validation, fault tolerance, and coherence.
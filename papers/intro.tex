%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
% \ugh{ASSIGNED TO: Amanda}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \ac{Outline:
% \begin{itemize}
%     \item Small blurb about hardware trends. Hardware is moving towards dense configurations of cheap memory (i.e., disaggregation). (tons of citation)
%     \item Small blurb about software programming models. With access to ample amounts of RAM, \textbf{shared} memory is a more viable model for certain types of computation. (citations)
%     \item How DSM fits into this? Distributed shared memory can allow applications to view a dense memory server as one chunk of memory and operate on it concurrently. (again small)
%     \item Original challenges of DSM --> complexity of management, concurrency, etc. 
%     \item We propose simplifying the DSM by moving the management of the system into the switch/network. To do so, we address all memory in IPv6-compliant addresses.
%     \item The advantages of this methodology is minimal overhead (directory service shittttt), simpler to move or redirect memory.
% \end{itemize}}

Distributed shared memory (DSM) is already a well explored field~\cite{gms, treadmarks, farm, grappa, Protic1996, Nitzberg1991}, showing benefits of scale, improved programmability, and cost effectiveness. DSM systems attempt to reduce complexity of distributed or parallel programming by providing a single address space for multiple processes to use. It handles all communication semantics and management of the memory. This scales well as more memory/nodes can be added easily depending on the system design. The improved programmability of the system comes from the single virtual shared memory address space it provides and the shielding of programmers from often complex communication and management implementations. DSM systems are also cost effective as, at the time, multiprocessor hardware or hardware which could create a shared address space was expensive. \cite{Protic1996}

% Queue DSM. Distributed shared memory is a well explored field...\cite{gms}
% \ac{Start describing the benefits of DSM (stolen from wikipedia), make it short
% \begin{itemize}
%     \item Scales well with a large number of nodes
%     \item Message passing is hidden
%     \item Can handle complex and large databases without replication or sending the data to processes
%     \item Generally cheaper than using a multiprocessor system
%     \item Provides large virtual memory space
%     \item Programs are more portable due to common programming interfaces
%     \item Shield programmers from sending or receiving primitives
% \end{itemize}
% }
But DSM never managed to reach widespread adoption due to complexity of tracking memory locations, overheads of communication, and complexity of implementing concurrency management~\cite{Li1989}. The implementation of application specific messaging proved to be a better trade-off than using a general distributed shared memory system. Writing application specific messaging allows the programmer more control over the communication protocol, data movement, and performance of the system. 

So why even bother discussing DSM systems? DSM systems need to be revisited because the traditional programming model which rendered DSM systems impractical is changing. Current datacenter programming models are shifting from traditional servers to rack-scale disaggregated servers. Disaggregated datacenters partition racks into compute units and storage units. The compute unit can be a server with multiple CPUs or GPUs and a small amount of local memory. The storage units are servers with a small CPU and a large array of SSDs, disks, or RAM.~\cite{Li2017}

The change in programming model allows for applications to easily leverage large amounts of remote memory. Unfortunately, most applications are written for servers which comprise of some memory and a reasonably adequate CPU. Disaggregation causes the memory units to have small CPUs and the CPU units to have small amounts of memory, something typical applications are not designed for. In-memory computation, latency sensitive systems, and real-time analysis systems all require large amounts of memory and fast compute. These systems could become easier to program and design if they could leverage the disaggregated memory.  
% Denser configurations change how applications can efficiently access or utilize the memory. In-memory applications could benefit from the large amount of shareable remote memory. 
% For example, a graph processing algorithm might require each individual computation process to access multiple vertexes. Instead of requiring the data to be passed among the computations, each computation can access a shared virtual address space. This idea is not new, rather it is identical to the idea that drove most distributed shared memory work. 
Considering the change in programming model brought on by datacenter trends, it is imperative to revisit distributed shared memory in this context. But, the complexities surrounding building DSM systems still exist in this new context.

To mitigate the management complexities inherent to DSM systems, we propose BlueBridge, a network managed memory system. BlueBridge leverages advancements in automatic network configurations with IPv6 addressed memory at the core of its design. By storing every memory address in an IPv6-compliant manner, the location information of remote memory is encoded in the IPv6 packet header, which makes it available to the switch when routing. Software-defined networking (SDN) allows the controller of the switch to have a global view of the network, thus a global view of the memory system. It can determine hot spots, migrate memory, etc. by installing rules on the switch or monitoring the traffic on the switch.

% By requiring the switch to manage the memory, there are many benefits. For example, if memory moves or a server fails and a replica takes over, the client machine does not need to be aware and get a new location. The switch can reroute the request to a different port with a different flow rule. 
This design eliminates migration complexity, the switch can re-route the address to the new machine, and reduces the amount of management communication. Typical distributed shared memory systems always require a directory service to determine where the memory is stored. Requiring the switch to act as this directory service removes an extra roundtrip time. It does not directly solve the complexity surrounding concurrent access, but, with a global view of the system, the controller can implement policies to make reasoning and solving concurrency less complex.

We realize this design in a proof of concept implementation, which shows that IPv6-compliant addressing is possible and can be used to access remote pages during a page fault. We evaluate this implementation by exploring the breakdown of latency per page fault and comparing the latency to local disk. We find that by moving the directory service into the switch, we can improve the latency by 2x and, by leveraging RDMA communication, we can perform a remote page fault in a similar latency to a local disk page fault. These numbers are not concrete due to the proof of concept nature of our implementation, but they show the promise of integrating the network with the shared memory management.
% Advantages?
% \begin{itemize}
%     \item Minimal overhead
%     \item Fast fail-over
%     \item Agnostic client and servers \fr{only in the management sense}
%     \item Abstraction reduces complexity
% \end{itemize}

Our paper is organized as follows. First we discuss the related works for this project in Section \ref{sec:related}. Next, we discuss the full design of BlueBridge in Section \ref{sec:overview}. Section \ref{sec:implementation} details our proof of concept implementation, and Section \ref{sec:eval} evaluates that proof of concept. Finally we conclude with future works in Section \ref{sec:conclusion}. 
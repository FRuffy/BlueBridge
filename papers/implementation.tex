\section{Proof of Concept System}
\label{sec:implementation}
To evaluate the practicality of the aforementioned design we built a simple proof-of-concept tool operating entirely on IPv6 memory addressing. The implementation is minimal and focuses on appropriate conversion and network transport techniques to demonstrate the basic IPv6 shared memory use case and its benefits. The virtual memory system as well as the control plane and consistency management properties of the BlueBridge design are left as future work.
The client-server program is written in C and Python and utilizes basic C memory operations.
\subsection{System Aspects}
% The general operational structure of the tool is already reminiscent of the full design proposal.
The client consists of a library of simple remote allocate, read, write, and free functions which are mirrored on the server side. Allocation is chosen at random out of a list of possible host numbers. Instead of \texttt{mmap()}, the client stores acquired pointers in a simple list structure to read and write its data. When accessing remote memory the client inserts the IPv6 pointer into the header and sends it using a Linux datagram socket. The server retrieves the destination IP section, converts the 128 IPv6 address into a 64 bit pointer, and faithfully performs the requested action. We currently do not check for invalid or corrupted pointer addresses, which may lead to errors on the server side. A fully asynchronous, non-blocking client and server programs as well as checking pointer addresses are functions left for future work

\subsection{Networking Aspects}
We run our proof of concept system in Mininet~\cite{mininet}, a networking virtualization engine primarily used for SDN research.
% It enables the testing of networking configurations on a single machine by encapsulating nodes in a custom, dedicated network container.
% Mininet supports virtual switches as well as custom controllers, making it a prime candidate to conduct our tests.
Our current network setup consists of a single switch and controller managing up to 42 nodes. We are running a simple Open vSwitch which is pre-configured over several static \texttt{add-flow} commands which specify the appropriate server subnets. 
% Although the system is capable of running in conjunction with a controller (here: Ryu), we are not yet relying on it. The controller would only operate in reactive mode and not install any routing entries. 
All forwarding is performed on layer two only and all flow entries are MAC-based. As these type of operations can be just as effectively covered by the static configuration, we have decided to not include the controller in this prototype.We leave controller integration to future work.

We still rely on the Linux network stack for communication, which automatically configures all necessary operations below transport layer. This includes neighbor discovery, packet construction, route entry management, and packet forwarding.
Every pointer is a unique IPv6 address and hosts do not reply to unknown addresses. As a consequence, our system needs dedicated routing table entries for each node and utilize the network discovery protocol to function properly.
As it is impossible to specify IPv6 subnet proxies natively, every node runs a NDP-proxy server which responds to NDP solicitation requests. For every pointer that is sent out into the network, a NDP solicitation request is broadcast. The proxy of the owning server will respond and the appropriate routing entry is created on all relevant nodes.
This behaviour is highly undesirable as it generates round-trip overhead, nearly doubles network traffic, and may lead to peculiar problems (see Section~\ref{sec:silly_ndp}). It is thus mandatory to abolish the dependence on NDP for our system to succeed. As it is a default feature of Linux transport layer operations, we are planning to move to raw socket programming entirely. This also provides us with the benefit of high customizability and the potential to define a leaner, specialized protocol.

\subsection{IPv6 Remote Paging}
% \ac{Amanda please work on.}
% \ac{Done to be similar to DSM systems work, see \cite{Protic1996}}
We also created a \texttt{userfaultfd} program to perform remote paging, reminiscent of how DSM systems would transparently retrieve remote memory for an application. When a page fault occurs, the system determines if the page is remote or local. If the page is remote, it performs whatever communication is necessary to retrieve it, if it is local, it fetches the page. \cite{Protic1996} In our implementation, we moved most of the \texttt{client} logic into \texttt{userfaultfd}. The program \texttt{mmap}s $n$ number of addresses into memory marked to be handled by the \texttt{userfaultfd} handler. When it \texttt{mmap}s the memory, it allocates remote memory on the \texttt{server} and maintains a mapping of local addresses to remote addresses. Every access causes a page fault which is handled by our handler. We then forced page fault accesses to these pointers. For each page fault, the program checks to see if the pointer is remote or not by checking a hardcoded map. If the pointer is remote, it performs a READ operation on the remote memory and loads that into the local pointer.

% After building the program we conducted several tests to evaluate its current efficiency and to identify potential shortcomings and bottlenecks.

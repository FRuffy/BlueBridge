\section{Proposals}
\label{sec:proposal}
In this section we propose experimental features of a disaggregated
memory system, and extensions to an existing distributed shared memory
system which will allow developers to write more expressive
applications on top of it.


\textbf{Disaggregated memory striping.} In a disaggregated
rack the memory allocated to a single application may be resident on
separate machines. This raised the question \emph{What are the operational
semantics for a failure of part of a memory allocation?} A variety of
options are available; The traditional approach would see the
application share the fate of its memory, and simply crash. In
contrast the contents of the memory could be replicated, and in the
face of a failure, a memory replica is transitioned to transparently.
We propose to use an approach already common in disks, mainly the RAID
architecture.

Our proposal is to implement main memory raiding at the page level.
When an application page fault occurs, the page fault is intercepted
by a raiding manager which implements a given raid algorithm, either
(0, 1 or 6), and stripes the evicted page across memory blades. To
retrieve pages, the page manager requests the striped page from memory
blades and checks for the pages correctness. In the case of a failure,
our proposed system has the same semantics as traditional raid. If a
page is missing, it is reconstructed using parity raid 2-6. Or in the
case of raid 0 the data is lost. In the face of truly lost data (raid
0 or 1) an entire application should share the fate of the memory
blade.

Using main memory raid has many benefits. In addition to correctness
in the face of partial failures, bit level memory correctness is also
checked.  This allows for optimization's to be made at the network
later. For example remote pages need not use TCP checksums. Finally
this architecture has the advantage that memory bandwidth is
multiplied almost linearly by the number of raided memory blades.

To evaluate our disaggregated memory raided system we will implement
popular graph processing algorithms (label propagation, and pagerank)
and measure their performance relative to benchmarks reached by other
frameworks, such as Naiad, and GraphX. Additionally we will measure
the rate of recovery of applications in the face of failures. 

\textbf{Multi-threaded applications.} To date our
distributed memory simulator only has support for single threaded
applications, which share no resources. To investigate the appropriate
OS semantics for applications written on a disaggregated architecture
memory sharing and multi-threading are necessary. Adding these
additional features requires re-engineering a portion of Blue Bridge.

In the current architecture a single thread is allocated a local
scratch space, when the scratch space is filled, and an application
page faults, a request for distributed memory is made. The request has
no information about the thread which issued the request or the scope
of its distributed memory access. We propose an extension to the
distributed memory allocation protocol, in which requests are
identified by their memory offset, and the thread which issued the
request. Additionally the system will require extensions to the memory
manager so that multiple local scratch spaces can be maintained
concurrently.

The addition of multi threaded applications raises the problem of
isolation. Currently BlueBridge gives a single thread unrestricted
access to a 128 bit address space. To implement isolation, a manager
must maintain an offset on which a given thread can operate on all
memory blades. In essence this extension is simplified virtual memory
for a distributed system.

With the addition of isolation comes the need to facilitate sharing.
We propose the use of named segments at known locations to allow
sharing between isolated threads. Additionally two threads will have
the ability to operate in the same address space for efficient
computation.

\textbf{LRU caching.} BlueBridge's current implementation
makes use if FIFO page replacement. This paging strategy was chosen
for simplicity during early development; now it represents a
performance bottleneck, and a more optimal strategy should be used. We
propose the implementation of an efficient LRU page replacement
algorithm, implemented in two stages. First the LRU cache eviction
will be implemented on client nodes. LRU maintenance will be done when
page faults occur on local pages, and remote requests are issued. The
second stage of the LRU implementation will be to move the caching
logic to a virtual programmable switch in MiniNet.

Our LRU implementation should be efficient. We propose the use of a
linked hash map, for $O$(1) access and update. This implementation may
be too memory intensive to run on a switch given the large address
space and limited memory on the switch. If resources are too scarce
for this implementation, LRU can be implemented efficiently using
splay trees.

\textbf{Switch MMU.} Access to distributed memory is
currently administered by a user space process running on another layer of
virtual memory. In case of a page fault on the allocated buffer, the application
traps into the handler, which performs address translation and issues requests
for remote memory. While simple, this approach does not take advantage of the
IP-addressing schema.

In a rack-scale design, all memory requests pass the central ToR-switch to reach
any remote node. Since we are encoding memory access directly in packet headers,
we can leverage the switch as flexible enforcement agent. Using the programmable
data plane language P4 we are able to define our own switch control flow and
redefine packet processing behavior. Switches may interact with a central
directory service or controller defining custom policies. This allows us to
investigate several potentially interesting questions of the system.
\begin{enumerate}
\item The client-server interaction of BlueBridge resembles a write-back cache.
Assuming servers are cooperative, could we implement a MESIF-style protocol
directly in the network data path and manage cache coherence and consistency?
\item A central scheduler and directory service may manage the distribution and
balancing of memory in the system. Is it possible to transparently migrate
memory by rewriting and redirecting client requests?
\item Are there benefits in performing address translation in the switch instead
of in the virtual memory layer? Can we implement a transaction lookaside buffer
on switches, which will trap to a central directory service to translate requests?

\end{enumerate}
We intend to build the foundation for such a system and develop a programmable
switch and controller blueprint, which will be tested in the Mininet emulator.
If time permits, we plan to explore one of the aforementioned ideas and evaluate
their effectiveness.

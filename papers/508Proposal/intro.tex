%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TO DOS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction} 
\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% DCC is here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Today's datacenters (DCs) are server-centric: users rent servers with
specific hardware capabilities tailored to their needs (e.g.,
compute-intensive EC2 instances from Amazon). 
A \emph{disaggregated} datacenter (DDC) disaggregates, or separates,
the resources in a traditional DC into resource \emph{blades},
with each resource connected directly to an interconnect (Figure~\ref{fig:DDC}). 
We use the term \emph{blade} to describe a 1U server containing one resource
type, and \emph{resource} for an individual CPU, DIMM, SSD, etc. in a blade. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% DDC benefits %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The modularity of DDCs benefits both operators and
users~\cite{Han2013}. By separating resources into blades, a
DDC provides efficiency: the operator can upgrade specific hardware
blades without impacting other resources types. Users also experience
an efficiency win: in a DDC a user can provision the exact amount of
resources they require and dynamically expand/shrink this set. This flexibility
also increases the DC resource utilization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Research challenges %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% There are two broad types of disaggregation: partial and full.
% In partial disaggregation compute resources have a small amount of local memory,
% memory and storage resources have CPUs, and a NIC is attached to each resource. 
% In full disaggregation each resource is independent and is directly
% connected to the rack interconnect. Current research focuses on the practicality
% of partial disaggregation~\cite{IntelRSA,Gao2016,Lim2009,Lim2012}.

% In this paper, we discuss partial disaggregation at rack-scale:
% resources can only connect to other resources in the same rack (e.g., a CPU in
% rack1 cannot connect to a DIMM in rack2). The disaggregated rack is presented to
% the client as a single machine through a virtualization layer. Applications can
% request a number of VMs to allocate among the racks. Distributed applications
% might then run among several disaggregated racks, while a single-machine
% application will reside in one rack.

Disaggregation also challenges several assumptions behind datacenter software
design: as memory becomes rack-wide and shared between rack CPUs, the entire
rack starts to resemble a ccNUMA system rather than a set of traditional
distinct servers. This changes assumptions surrounding fate sharing boundaries
and the role of the network.

The idea of a globally shared address space is not new \todo{CITE STUFF}. A
globally shared address space allows applications to scale across a cluster and
perform computations on data without the need of partitioning. Yet, single
global address space systems historically do not gain traction due to the
implications of transparent distribution: poor performance and difficult to
understand failure models.

ccNUMA machines also encounter these same issues unless applications are
written explicitly for non-uniform access of memory. Scalable
single-host systems are often designed with each core acting as a
``shared-nothing'' instance. Essentially, each core operates independently on
partitioned data and local structures. 

As disaggregation pushes racks to appear more like a single ccNUMA machine and
ccNUMA machines trend towards looking like distributed systems,
the question arises: \emph{What would it take to allow us to treat a rack of
disaggregated resources like a supercomputer with commodity components?} In
particular, we plan on extending the \texttt{BlueBridge} distributed memory
system to support remote memory sharing, memory RAIDing, optimized paging with
different cache eviction policies, and a programmable switch which acts as the
MMU.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{fig/ddc-overview}
    \caption{Simplified representation of a disaggregated datacenter with
    three communication points: (1) between the application and the
    virtualization layer,
    (2) between the virtualization layer and the physical resources, and (3)
    between the physical resources bundled as blades.}
    \label{fig:DDC}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

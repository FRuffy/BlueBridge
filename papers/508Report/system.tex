\section{System}
\label{sec:system}
\todo{Needs proofreading}
\subsection{Prior System} Camelot is a successor to a
simplistic DSM system. In the prior system, "BlueBride" all clients to the
system have an unrestricted view of the entire virtual address space.
Client program were restricted to a single thread, and all paging was
performed using a naive FIFO paging strategy. BlueBride made use of a
novel 128 bit IPv6 remote addressing mechanism which we adopted into
Camelot. Remote addresses are represented as 128-bit IPv6-compatible
pointers. On the client side, all remote addresses are stored as
combination of the virtual pointer provided by the service node and
its source IP address. If a client accesses remote memory, it inserts
the address into the IP header of the request packet. The switch will
route it automatically to the correct server node.

\subsection{Network and Virtual Memory Stack}
To support custom protocols without being dependent on the Linux networking stack, Camelot uses its own network library. The library is implemented using raw sockets, Berkely Packet Filters (BPFs)~\cite{bpf} and \texttt{packet mmap}~\cite{packet_mmap}.
At runtime, the application allocates a shared ring buffer, which it registers with the kernel.  After binding the socket to an interface, the kernel copies any event into the receive buffer and continuously dispatches packets in the send buffer. To reduce the amount of packets inserted into the ring buffer, applications are able to insert a BPF byte-filter code, which directs the kernel to only copy relevant packets. This approach is substantially faster than conventional Linux send and receive, as it allows us to remove unnecessary procedures and define a custom control flow.

Camelot also uses a cache management system to provide users with a section of managed memory. Users interfacing with the system initialize the virtual memory manager with a call that specifies cache and global address space size. The call will return a pointer to the managed cache memory, behaving like a typical C \texttt{malloc()}. The global memory space is allocated via \texttt{mmap()} and completely protected. Faulting calls are subsequently mapped to the frame cache. This forms the page table of the system. 

\subsection{Page Eviction} 

\subsection{Threading} 
As a distributed shared memory system Camelot must support multithreading, simply for multi-core support and the ability to explore sharing cache coherence semantics.
In order to support multithreading, the network as well as virtual memory stack have to be thread-safe.
\subsubsection{Threading in the Network library}
Initially, threads were sharing a single ring buffer and individually consumed the packets placed by the kernel. Ideally, a thread would loop over the buffer, pick and discard relevant packets, and ignore packets of other threads. Frequently however, threads were not scheduled in time, causing their packets in the buffer to be displaced or dropped. The only apparent, immediate fix to this problem seems to be increasing the ring buffer size.
In the final version of the system, only minimal sharing occurs. Each thread manages their own buffer which it has bound to the interface. To avoid unnecessary copy overhead thread register a packet filter and maintain their own UDP port.

\subsubsection{Threading in the Virtual Memory System}
In the virtual memory system,  sharing, too, is minimal. Page metadata and the frame table are global, but sliced in subsets. Each thread owns a slice of the frame cache, which it has full control over. Threads are only able to access frames in their own cache, and are unable to evict or fill pages of other threads. While accessing and sharing pages of the global address table is theoretically possible, no support for coherence and consistency exists yet. 

\subsection{Fault Tolerance} 

Camelot achieves fault tolerance via in memory RAID. Thus far we have
implemented 4 RAID variants 0,1,4,5 which provide each RAID's fault
tolerant guarantees. RAID 0 is data striping only, and provides no
fault tolerance. The failure of a single memory server causes
applications to hang, this variant was developed as a baseline. RAID 1
is full replication, applications running RAID 1 can survive up to
$n-1$ memory node failures. This RAID has the same guarantees as
RAMCloud and was developed for the sake of comparison. RAID 4 \& 5
use erasure encoding, in the form of parity to gain the tolerance of
a single memory server failure. In RAID 4 parity is resident on a
single machine, whereas RAID 5 distributes parity across all servers.

Camelots RAID manager resides in its page fault handler on client side
applications. When a page fault occurs the RAID manager performs a
RAID variant, as specified by a configuration file. In the case of
RAID 0 with 4 memory servers the manager manager splits the evicted
4096 byte page into 1024 byte stripes and issues a separate async
request to 4 servers. Upon receiving 4 acks the RAID manager returns
control to the user level application. The same protocol holds for
RAID 1, although full pages are broadcast to all servers. When a page
is requested in RAID 0, all servers must respond with their stripe to
compose a page, whereas RAID 1 returns control to applications after
receiving its first replica.

RAID 4 and 5 require parity calculation on page evictions, and
repair computation when failures occur. Parity calculation, and
repair of 4K pages takes between 10 and 15 microseconds. The main
bottleneck of these operations is cache memory bandwidth, as the
calculation itself only requires the execution of C's XOR function. In
the failure case when a page from a failed node is not received
correction is run on the fly and repaired pages are passed to the
application so it can operate opaque to failures. In such cases
detecting that a page is missing via timeout is the dominant factor
which increases latency. Our current implementation uses a fixed
timeout of 100 microseconds, and no failure detector has been
implemented as of yet. In the future using a dynamic timeout, or
failure detector could dramatically decrease the cost of running
repairs.



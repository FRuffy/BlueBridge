\section{Introduction} 
\label{sec:intro}
As the amount of digital data collected increases, the need for data-intensive
large scale processing increases. For example, Facebook processes over 500
terabytes a day and has invested in a large data processing pipleline to process
that data \todo{Recent facebook paper on streaming pipeline.}
\todo{https://techcrunch.com/2012/08/22/how-big-is-facebooks-data-2-5-billion-pieces-of-content-and-500-terabytes-ingested-every-day/}. 
Large scale data processing is not limited to large technology companies,
different disciplines in the sciences have also taken advantage of computing
power to make advancements. For example, high performance computing has been
used to improve processing times of electron microscopy images and super
computers have been used to implement and develop DNA sequencing systems.
\todo{http://www.sciencedirect.com/science/article/pii/S030439911630300X, https://www.osc.edu/research/report10/BiologicalSciences/DNAsequencing}

Previous work has looked at solving the issue of large scale data processing for
different types of applications (i.e., graph processing, streaming, etc.).
Distributed processing frameworks often have to make the trade off between
performance and generality by enforcing certain abstractions or programming
models on the developer~\cite{Dean2004,Low2010,Murray2013}. 
This, unfortunately, restricts the types of applications which can run on these
frameworks, often leading to developers attempting to force their workload into
one of these frameworks. If a developer wishes to use Hadoop, they must write
their algorithm in the MapReduce paradigm~\cite{Dean2004}. Or, if they wish to
run graph processing on GraphLab, they must adhere to the vertex-centric
processing paradigm~\cite{Low2010}.

Yet, it has been shown that many of these frameworks out performed by a single
threaded laptop implementation~\cite{189908}. McSherry showed that his
implementation of certain processing algorithms in RUST outperformed the most
popular frameworks for that algorithm. But, programming highly performant code
requires knowledge of the system (i.e., operating system tuning) and is not
necessarily extensible  (different tuning parameters per dataset and algorithm).
The application developer needs to be able to reason about their algorithm but
also not reason about the underlying system to gain performance. 

New trends in technology make it possible to have a combined approach, where a
distributed framework abstracts away all complexity associated with distribution
and presents the user with a single NUMA machine interface. We envision a system
which turns a datacenter rack or equivalent cluster into a large NUMA machine.

We realize the first part of this system in Camelot, the network managed
distributed shared memory backend. 

Distributed shared memory (DSM) is made realistic by faster interconnects.
Traditionally, DSMs did not achieve adoption due to low throughput and high
latency of network data transfers \todo{cite}. Now, with remote direct memory
access (RDMA) and faster networks, the prospect of achieving low-latency (3 - 5
microsecond) is much more attainable. Due to this, there has been a resurgancy
of research into DSM systems and remote memory systems \todo{cite}. Recent work
has attempted to address the issues surrounding DSMs even with fast
interconnects, such as fault tolerance~\cite{Ongaro2011,Nelson2015}.

The one closest to our work, Grappa, exposes a single shared memory space with
a simplified interface to allow for expressive but simple applications to run
on the system. Although, both systems, our vision and Grappa, attempt to provide
the same service (simple expressible interface on DSM), we provide optimizations
and features beyond Grappa. We move memory management into the network to reduce
the amount of network traffic and improve performance. Camelot also provides
fault tolerant memory by adding different RAID levels to the system. 

Camelot is the next generation of a previous system, BlueBridge, which
implemented the basis for network managed DSM. We expand upon BlueBridge to
bring it closer to our envisioned system by adding support for multi-threading,
improved paging policies, and fault tolerance.

The contributions of this work are as follows:
\begin{itemize}
	\item We added multi-threading support to BlueBridge, which allows for
	mulithreaded applications to take advantage of the DSM.
	\item We implemented different paging policies in Camelot to analyze the
	effectiveness of each policy to determine the best one for Camelot.
	\item We added fault tolerance by integrating RAID to the end hosts\ac{Stew,
	is it at the end host (server) or the client?}.
\end{itemize}

In the next section we will discuss motivating use cases for this type of
framework. 

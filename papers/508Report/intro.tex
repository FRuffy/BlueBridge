\section{Introduction} 
\label{sec:intro}
As the amount of digital data collected increases, the need for data-intensive
large scale processing increases. For example, Facebook processes over 500
terabytes a day and has invested in a large data processing pipleline to process
that data~\cite{Chen2016}. Large scale data processing is not limited to large
technology companies, different disciplines in the sciences have also taken
advantage of computing power to make advancements. For example, high performance
computing has been used to improve processing times of electron microscopy
images and super computers have been used to implement and develop DNA
sequencing systems~\cite{Puckelwartz2017,Oelerich2017}.

Previous work has looked at solving the issue of large scale data processing for
different types of applications (i.e., graph processing, streaming, etc.).
Distributed processing frameworks often have to make the trade off between
performance and generality by enforcing certain abstractions or programming
models on the developer~\cite{Dean2004,Low2010,Murray2013}. 
This, unfortunately, restricts the types of applications which can run on these
frameworks, often leading to developers attempting to force their workload into
one of these frameworks. If a developer wishes to use Hadoop, they must write
their algorithm in the MapReduce paradigm~\cite{Dean2004}. Or, if they wish to
run graph processing on GraphLab, they must adhere to the vertex-centric
processing paradigm~\cite{Low2010}.

Yet, it has been shown that many of these frameworks out performed by a single
threaded laptop implementation~\cite{189908}. McSherry showed that his
implementation of certain processing algorithms in RUST outperformed the most
popular frameworks for that algorithm. But, programming highly performant code
requires knowledge of the system (i.e., operating system tuning) and is not
necessarily extensible  (different tuning parameters per dataset and algorithm).
The application developer needs to be able to reason about their algorithm but
also not reason about the underlying system to gain performance. 

New trends in technology make it possible to have a combined approach, where a
distributed framework abstracts away all complexity associated with distribution
and presents the user with a single NUMA machine interface. We envision a system
which turns a datacenter rack or equivalent cluster into a large NUMA machine.
We realize the first part of this system in Camelot, the network managed
distributed shared memory backend. 

Distributed shared memory (DSM) is made realistic by faster interconnects.
Traditionally, DSMs did not achieve adoption due to low throughput and high
latency of network data transfers~\cite{Li1989}. Now, with remote direct memory
access (RDMA) and faster networks, the prospect of achieving low-latency (3 - 5
microsecond) is much more attainable. Due to this, there has been a resurgancy
of research into DSM systems and remote memory systems~\cite{Ongaro2011,Nelson2015,Dragojevic2014}. 
% Recent work
% has attempted to address the issues surrounding DSMs even with fast
% interconnects, such as fault tolerance~\cite{Ongaro2011,Nelson2015}.

The recent work closest to Camelot, Grappa, exposes a single shared memory
space with a simplified interface to allow for expressive but simple
applications to run on the system. Although, both systems, our vision and
Grappa, attempt to provide the same service (simple expressible interface on
DSM), we provide optimizations and features beyond Grappa. We move memory
management into the network to reduce the amount of network traffic and improve
performance. Camelot also provides fault tolerant memory by adding different
RAID levels to the system. 

Camelot is the next generation of a previous system, BlueBridge, which
implemented the basis for network managed DSM. We expand upon BlueBridge to
bring it closer to our envisioned system by adding support for multi-threading,
improved paging policies, and fault tolerance.

The contributions of this work are as follows:
\begin{itemize}
	\item We added multi-threading support to BlueBridge, which allows for
	mulithreaded applications to take advantage of the DSM.
	\item We implemented different paging policies in Camelot to analyze the
	effectiveness of each policy to determine the best one for Camelot.
	\item We added fault tolerance by integrating a RAID manager into the client side page fault handler.
        
\end{itemize}

We evaluated each improvement to the system to determine the effects on
performance. We found \todo{insert findings Fabian Bronson
\sout{Stew}}. In memory RAID can significantly reduce memory usage
(over 50\% on common configurations) with a computational overhead of \textasciitilde6\%
compaired to competing solutions~\cite{Ousterhout:2015:RSS:2818727.2806887}.

Our paper will be organized as follows. First we will discuss previous work in
the space in Section~\ref{sec:related}. Then we will discuss the design and
implementation of Camelot in Section~\ref{sec:system}. Section~\ref{sec:eval}
details the evaluation methodology and results. Finally, we conclude with
discussion and future work. 

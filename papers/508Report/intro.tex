\section{Introduction} 
\label{sec:intro}

\todo{Introduce rack-scale + faster interconnects part sooner to setup claim
surrounding management in the network.}

\ac{1. Data-intensive applications important}

As data collected increases, the need for data-intensive large scale processing
increases. For example, Facebook processes over 500 terabytes a day~
\todo{https://techcrunch.com/2012/08/22/how-big-is-facebooks-data-2-5-billion-pieces-of-content-and-500-terabytes-ingested-every-day/}. Large scale data processing is not limited to large
technology companies, different disciplines in the sciences have also taken
advantage of computing power to make advancements. For example, high performance
computing has been used to improve processing times of electron microscopy
images and super computers have been used to implement and develop DNA
sequencing systems.\todo{http://www.sciencedirect.com/science/article/pii/S030439911630300X, https://www.osc.edu/research/report10/BiologicalSciences/DNAsequencing}

\ac{2. single machine vs distributed}

The main dichotomy
which appears in large scale data process is single machine (high performance
computing) vs. clustered machines (distributed computing). Single machine
computing has an advantage in locality of data and compute, but has the
disadvantage of cost. Clustered computing on commodity hardware is cheaper, but
has the disadvantage of added complexity\footnote{This is not to imply that
programming HPC applications tend to be easier, they have their own set of
complexities, but they do not deal with complexities brought on by the
transference of data and compute over the network}. 

\ac{3. Previous distributed work}

A large amount of previous work attempts to reduce the complexity of
distributed processing by imposing abstractions~\cite{Dean2004,Low2010,Murray2013}.

\ac{4. previous work trade-off generality for performance.}

This, unfortunately, restricts the types of applications which can run on these
frameworks, often leading to developers attempting to force their workload into
one of these frameworks. If a developer wishes to use Hadoop, they must write
their algorithm in the MapReduce paradigm~\cite{Dean2004}. Or, if they wish to
run graph processing on GraphLab, they must adhere to the vertex-centric
processing paradigm~\cite{Low2010}.

\ac{5. Single machine implementations require programmer expertise to take
	advantage of optimizations (COST + HPC) which is not always guaranteed.}

Yet, in the single machine case, it has been shown that
many of these frameworks out performed by a single threaded laptop
implementation \todo{cite COST}. McSherry showed that his implementation of
certain processing algorithms in RUST outperformed the most popular frameworks
for that algorithm. But, programming highly performant code requires knowledge
of the system (i.e., operating system tuning) and is not necessarily extensible 
(different tuning parameters per dataset and algorithm). The average programmer
needs to be able to reason about their algorithm but also not reason
about the underlying system to gain performance. 

\ac{6. We envision a transparent cluster based rack-scale NUMA machine.}

We propose a combined approach where the advantages of the single machine
implementation can be combined with the advantages of the distributed
implementation. We envision a system which provides a simple single threaded
interface for application developers to run on and underlying
system parallelizes and distributes that application as appropriate.
Essentially, we hope to expose a single rack-scale NUMA machine to the
application developer. 

\ac{7. Introduce Camelot}

We realize the first part of this system in Camelot, the network managed
distributed shared memory backend. 

\ac{8. DSMs have been studied extensively. Why they coming up again?}

Distributed shared memory has be extensively studied in the past, but has
recently resurfaced as an promising area of research due to faster
interconnects. Traditionally, DSMs did not achieve adoption due to low
throughput and high latency of network data transfers as well as the added
complexity of fault tolerance. Recent work has attempted to address this issues
individually using different techniques~\cite{Ongaro2011,Nelson2015}.

\ac{9. compare contrast with grappa.}

Whereas Grappa attempts to solve the issue of performance with \todo{blah}. 
We differ from both solutions as we attempt to improve performance and fault
tolerance by leveraging the network. 

\ac{10. We expand upon a previous system, BlueBridge.}

Camelot is the next generation of a previous system, BlueBridge, which
implemented the basis for network managed DSM. We expand upon BlueBridge to
bring it closer to our envisioned system by adding support for multi-threading,
improved paging policies, and fault tolerance.

% \begin{itemize}
%         \item processing data at scale is hard
%         \item scientists, not just computer scientists process data
%         \item python, and C are more understood / natural than spark/hadoop/naiad
%         \item the state of current racks allow for comprable performance with DSM
%         \item (small contribution) These system admit failures, but they require expertise to reason about. Applications should be reasonably fault tolerant to prevent heartache, and confusion.
% \end{itemize}


\section{Related Work}
\label{sec:related}

There has been extensive work in shared and remote memory systems over the past
decades. We break related work into two broad categories: distributed shared
memory systems, fault tolerance in distributed shared memory systems, and
systems which expose a single machine interface.

\subsection{Distributed Shared Memory.} 
Distributed shared memory is an field of operating systems, which has been extensively
studied over the decades.
Two major reoccurring issues in shared memory are the performance impact of non-uniform memory 
accesses and consistency management. Often, these two aspects are at odds with each other.
Systems of note, TreadMarks~\cite{Keleher1994} and Cashmere~\cite{cashmere}, use different techniques 
to improve performance by trading off consistency. Treadmarks attempts to reduce
the amount of communication necessary to enforce memory consistency. Similarly, Cashmere is a 
``two-level'' shared memory system, which attempts to meet the challenge of performance by leveraging 
low-latency read-write capabilities. Camelot differs from both approaches in that we intend to attempt 
centralize shared memory management. Using a network addressing scheme, network elements are able to 
enforce consistency, reducing communication overhead and roundtrip time.   the switch to reduce 
network overhead.

% \begin{itemize}
%     \item treadmarks~\cite{Keleher1994} \todo{Fabian} \\
%     \item piccilo~\cite{piccolo} \todo{Fabian}  \\
%     \item CASHMERe ~\cite{cashmere} \todo{Fabian}  \\
%     \item Grappa~\cite{Nelson2015} \todo{Amanda}\\
% \end{itemize}

\subsection{Fault Tolerance in DSM} Prior work on fault tolerant main memory in a
distributed context was done by the RAMCloud project~\cite{Ousterhout:2015:RSS:2818727.2806887}. 
Their approach uses full replication a key-value store, similar to RAID1, and
disks which provide a durable log for the system to recover from.  Our approach
to fault tolerant memory requires less memory, at the cost of computation. We
implement RAID4/5 at the page level which requires that only a single replica
contains parity data while supporting the same level of fault tolerance as
RAMCloud.

\subsection{Single Machine Interface.} This section looks at two DSMs which most
closely resemble our system goals: a distributed framework which exposes a
simple single machine interface to the developer. Grappa, unlike previous DSM
work, relies on parallelism to achieve performance instead of relying on memory
locality~\cite{Nelson2015}. This similar to Camelot, as it does not rely on
locality for performance, but they differ in the since that Camelot attempts to
achieve that performance by moving management to the network and improving the
network transfer times. Piccolo exposes a key-value table interface to
computations running on different machines which acts as shared distributed
state~\cite{piccolo}. Piccolo is similar to the programming model we wish to use
in Camelot, but we envision an even more general programming model. Piccolo
requires the developer to organize their computation into application kernels
while reasoning about how those kernels will be distributed~\cite{piccolo}.
Camelot, on the other hand, will allow applications to be written in a
high-level, easy to understand language, such as Python, which provides more
expressibility to the developer.	

While entirely general we present Camelot in the context of a big data
processing framework. Systems such as Hadoop~\cite{Dean2004}, Pregal~\cite{Malewicz:2010:PSL:1807167.1807184}, 
Spark~\cite{180560} use embarrassingly parallel constructs suchas MapReduce, or
a \textit{think like a vertex} programming mentality. These constructs allow for
extreme scale but restrict programmers to their constructs and often produce
inefficient algorithms. In contrast Camelot allows developers to process big
data using conventional C. Efficient implementation of many algorithms can be
achieved using data parallel techniques found in Dryad~\cite{Isard:2007:DDD:1272996.1273005},
Naiad~\cite{Murray2013}, and TensorFlow~\cite{tensorflow2015-whitepaper}. These
frameworks have proven notoriously hard for developers to understand, and debug
in a distributed setting. Camelot's difficulty is exactly that of writing
multithreaded C.
